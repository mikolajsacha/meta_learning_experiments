{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "    \n",
    "def moving_average(l, n=10):\n",
    "    cumsum, moving_aves = [0], []\n",
    "    for i, x in enumerate(l, 1):\n",
    "        cumsum.append(cumsum[i-1] + x)\n",
    "        if i>=n:\n",
    "            moving_ave = (cumsum[i] - cumsum[i-n])/n\n",
    "            moving_aves.append(moving_ave)\n",
    "    return moving_aves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_train_train_loss, meta_train_valid_loss = [], []\n",
    "meta_train_train_acc, meta_train_valid_acc = [], []\n",
    "meta_valid_train_loss, meta_valid_valid_loss = [], []\n",
    "meta_valid_train_acc, meta_valid_valid_acc = [], []\n",
    "\n",
    "meta_train_loss_ratio = []\n",
    "meta_train_acc_ratio = []\n",
    "meta_valid_loss_ratio = []\n",
    "meta_valid_acc_ratio = []\n",
    "\n",
    "\n",
    "for meta in open('../log/meta_training_history.txt'): \n",
    "    meta = meta.split()\n",
    "    title = meta[0]\n",
    "    meta_results = meta[1].split(',')\n",
    "    if title == \"META_TRAIN_TRAIN:\":\n",
    "        meta_train_train_loss.append(float(meta_results[0]))\n",
    "        meta_train_train_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_TRAIN_VALID:\":\n",
    "        meta_train_valid_loss.append(float(meta_results[0]))\n",
    "        meta_train_valid_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_VALID_TRAIN:\": \n",
    "        meta_valid_train_loss.append(float(meta_results[0]))\n",
    "        meta_valid_train_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_VALID_VALID:\":\n",
    "        meta_valid_valid_loss.append(float(meta_results[0]))\n",
    "        meta_valid_valid_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_TRAIN_RATIO:\":\n",
    "        meta_train_loss_ratio.append(float(meta_results[0]))\n",
    "        meta_train_acc_ratio.append(float(meta_results[1]))\n",
    "    elif title == \"META_VALID_RATIO:\": \n",
    "        meta_valid_loss_ratio.append(float(meta_results[0]))\n",
    "        meta_valid_acc_ratio.append(float(meta_results[1]))\n",
    "        \n",
    "avg_n = 8\n",
    "\n",
    "meta_train_train_loss = moving_average(meta_train_train_loss, n=avg_n)\n",
    "meta_train_valid_loss = moving_average(meta_train_valid_loss, n=avg_n)\n",
    "meta_train_train_acc = moving_average(meta_train_train_acc, n=avg_n)\n",
    "meta_train_valid_acc = moving_average(meta_train_valid_acc, n=avg_n)\n",
    "meta_train_loss_ratio = moving_average(meta_train_loss_ratio, n=avg_n)\n",
    "meta_train_acc_ratio = moving_average(meta_train_acc_ratio, n=avg_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_conf_log = []\n",
    "i = 0\n",
    "for line in open('../log/train-meta-model.log'):\n",
    "    if line.startswith('2018'):\n",
    "        if i == 0 : i += 1\n",
    "        else: break\n",
    "    else:\n",
    "        training_conf_log.append(line)\n",
    "\n",
    "print('TRAINING CONFIGURATION:')\n",
    "print(''.join(training_conf_log))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "\n",
    "\n",
    "def draw_figure(train_data, valid_data, title, min_fun = None, train_label='train', valid_label='valid'):\n",
    "    train, = plt.plot(train_data, '-o', label=train_label)\n",
    "    legend_handles = [train]\n",
    "    if valid_data is not None:\n",
    "        valid, = plt.plot(valid_data, '-o', label=valid_label)\n",
    "        legend_handles.append(valid)\n",
    "    if valid_data is not None and min_fun is not None:\n",
    "        shift = 0.01\n",
    "        if min_fun is min:\n",
    "            shift *= -1\n",
    "        begin_handle, = plt.plot([0], [valid_data[0]], marker='o', markersize=8, color=\"orange\", label='starting')\n",
    "        ax = plt.gca()\n",
    "        ax.annotate('{:.5f}'.format(valid_data[0]), xy=(0, valid_data[0]), xytext=(-0.5, valid_data[0] + shift), \n",
    "                    horizontalalignment='left', verticalalignment='top')\n",
    "        \n",
    "        \n",
    "        best = min_fun(valid_data)\n",
    "        best_ind = valid_data.index(best)\n",
    "        best_handle, = plt.plot([best_ind], [best], marker='o', markersize=8, color=\"green\", label='best')\n",
    "        ax = plt.gca()\n",
    "        ax.annotate('{:.5f}'.format(best), xy=(best_ind, best), xytext=(best_ind - 0.5, best + shift), \n",
    "                    horizontalalignment='left', verticalalignment='top')\n",
    "        \n",
    "        legend_handles += [begin_handle, best_handle]\n",
    "\n",
    "    plt.legend(handles=legend_handles)\n",
    "    plt.title(title)\n",
    "    plt.show()    \n",
    "\n",
    "    \n",
    "draw_figure(meta_train_train_loss, meta_train_valid_loss,\n",
    "           \"Average meta-batch loss (meta-train, moving average n = {})\".format(avg_n), min)\n",
    "\n",
    "draw_figure(meta_train_train_acc, meta_train_valid_acc,\n",
    "           \"Average meta-batch accuracy (meta-train, moving average n = {})\".format(avg_n), max)\n",
    "\n",
    "draw_figure(meta_train_loss_ratio, None,\n",
    "           \"Average meta-batch ratio of valid loss to train loss (meta-train, moving average n = {})\".format(avg_n),\n",
    "           train_label='valid/train ratio loss ratio (meta-train)')\n",
    "\n",
    "draw_figure(meta_valid_train_loss, meta_valid_valid_loss, \"Average loss (meta-valid)\", min)\n",
    "\n",
    "draw_figure(meta_valid_train_acc, meta_valid_valid_acc, \"Average accuracy (meta-valid)\", max)\n",
    "\n",
    "draw_figure(meta_valid_loss_ratio, None,\n",
    "           \"Average ratio of valid loss to train loss (meta-valid)\".format(avg_n),\n",
    "           train_label='valid/train ratio loss ratio (meta-train)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for comparison of meta-optimizer with SGD and Adam\n",
    "\n",
    "import os\n",
    "from src.datasets.cifar import load_cifar100, cifar_input_shape\n",
    "from src.datasets.metadataset import load_meta_dataset\n",
    "\n",
    "from src.training.training_configuration import read_configuration\n",
    "from src.model.learner.simple_cnn import build_simple_cnn\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "train_conf_path = os.path.join(os.environ['CONF_DIR'], 'training_configuration.yml')\n",
    "conf = read_configuration(train_conf_path)\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_cifar100()\n",
    "\n",
    "meta_dataset_path = '../data/cifar100_64_64_2.h5'\n",
    "meta_dataset = load_meta_dataset(meta_dataset_path, X_train)\n",
    "\n",
    "learner = build_simple_cnn(cifar_input_shape, conf.classes_per_learner_set)\n",
    "\n",
    "learner_weights_path = os.path.join(os.environ['LOG_DIR'], \"learner_weights.h5\")\n",
    "learner.load_weights(learner_weights_path)\n",
    "initial_learner_weights = learner.get_weights()\n",
    "\n",
    "best_sgd_lr = 0.015\n",
    "best_adam_lr = 0.007\n",
    "\n",
    "learner.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.0))  # dummy optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best LR for SGD optimizer\n",
    "\n",
    "from src.utils.comparison import learning_rate_grid_search\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "trainings_per_dataset = 4\n",
    "\n",
    "sgd = SGD()\n",
    "lr_ph = tf.placeholder(shape=sgd.lr.get_shape(), dtype=tf.float32)\n",
    "lr_update = K.update(sgd.lr, lr_ph)\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "def sgd_factory(lr):\n",
    "    # set LR in tensorflow\n",
    "    sess.run([lr_update], {lr_ph: lr})\n",
    "    return sgd\n",
    "\n",
    "best_sgd_lr = learning_rate_grid_search(\n",
    "                 optimizer_factory=sgd_factory,\n",
    "                 meta_dataset=meta_dataset,\n",
    "                 lr_values=[0.01, 0.015, 0.02],\n",
    "                 n_learner_batches=conf.n_learner_batches,\n",
    "                 learner_batch_size=conf.learner_batch_size,\n",
    "                 learner=learner,\n",
    "                 trainings_per_dataset=trainings_per_dataset,\n",
    "                 initial_learner_weights=initial_learner_weights)\n",
    "\n",
    "print(\"Best SGD LR:\", best_sgd_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best LR for Adam optimizer\n",
    "\n",
    "from src.utils.comparison import learning_rate_grid_search\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "trainings_per_dataset = 4\n",
    "\n",
    "adam = Adam()\n",
    "\n",
    "# placeholders and tensors for updating Adam states\n",
    "iterations_ph = tf.placeholder(shape=adam.iterations.get_shape(), dtype=tf.int64)\n",
    "lr_ph = tf.placeholder(shape=adam.lr.get_shape(), dtype=tf.float32)\n",
    "beta_1_ph = tf.placeholder(shape=adam.beta_1.get_shape(), dtype=tf.float32)\n",
    "beta_2_ph = tf.placeholder(shape=adam.beta_2.get_shape(), dtype=tf.float32)\n",
    "decay_ph = tf.placeholder(shape=adam.decay.get_shape(), dtype=tf.float32)\n",
    "\n",
    "iterations_update = K.update(adam.iterations, iterations_ph)\n",
    "lr_update = K.update(adam.lr, lr_ph)\n",
    "beta_1_update = K.update(adam.beta_1, beta_1_ph)\n",
    "beta_2_update = K.update(adam.beta_2, beta_2_ph)\n",
    "decay_update = K.update(adam.decay, decay_ph)\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "updates = [iterations_update, lr_update, beta_1_update, beta_2_update, decay_update]\n",
    "\n",
    "def adam_factory(lr):\n",
    "    # set LR and reset states in tensorflow\n",
    "\n",
    "    # default Adam values\n",
    "    sess.run(updates, {iterations_ph: 0, lr_ph: lr, beta_1_ph: 0.9, beta_2_ph: 0.999, decay_ph: 0.0})\n",
    "    return adam\n",
    "    \n",
    "best_adam_lr = learning_rate_grid_search(\n",
    "                 optimizer_factory=adam_factory,\n",
    "                 meta_dataset=meta_dataset,\n",
    "                 lr_values=[0.0065, 0.007, 0.0075],\n",
    "                 n_learner_batches=conf.n_learner_batches,\n",
    "                 learner_batch_size=conf.learner_batch_size,\n",
    "                 learner=learner,\n",
    "                 trainings_per_dataset=trainings_per_dataset,\n",
    "                 initial_learner_weights=initial_learner_weights)\n",
    "\n",
    "print(\"Best Adam LR:\", best_adam_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.comparison import compare_optimizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from src.model.meta_learner.lstm_model import lstm_meta_learner\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "meta_model = lstm_meta_learner(learner=learner, hidden_state_size=conf.hidden_state_size,\n",
    "                               backpropagation_depth=conf.backpropagation_depth,\n",
    "                               initial_learning_rate=conf.initial_lr,\n",
    "                               debug_mode=conf.debug_mode)\n",
    "\n",
    "meta_model.predict_model.compile(loss='mae',  # we don't use loss here anyway\n",
    "                                 optimizer=SGD(lr=0.0))  # dummy optimizer\n",
    "\n",
    "best_meta_learner_weights_path = os.path.join(os.environ['LOG_DIR'], \"meta_weights_best.h5\")\n",
    "meta_model.load_weights(best_meta_learner_weights_path)\n",
    "\n",
    "meta_optimizer = meta_model.predict_model\n",
    "\n",
    "\n",
    "sgd = SGD()\n",
    "\n",
    "adam = Adam()\n",
    "iterations_ph = tf.placeholder(shape=adam.iterations.get_shape(), dtype=tf.int64)\n",
    "beta_1_ph = tf.placeholder(shape=adam.beta_1.get_shape(), dtype=tf.float32)\n",
    "beta_2_ph = tf.placeholder(shape=adam.beta_2.get_shape(), dtype=tf.float32)\n",
    "decay_ph = tf.placeholder(shape=adam.decay.get_shape(), dtype=tf.float32)\n",
    "\n",
    "iterations_update = K.update(adam.iterations, iterations_ph)\n",
    "beta_1_update = K.update(adam.beta_1, beta_1_ph)\n",
    "beta_2_update = K.update(adam.beta_2, beta_2_ph)\n",
    "decay_update = K.update(adam.decay, decay_ph)\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "def sgd_opt():\n",
    "    # no state to reset in SGD (we don't use momentum)\n",
    "    return sgd\n",
    "\n",
    "def adam_opt():\n",
    "    # default Adam state values (learning rate states constant)\n",
    "    sess.run([iterations_update, beta_1_update, beta_2_update, decay_update], \n",
    "             {iterations_ph: 0, beta_1_ph: 0.9, beta_2_ph: 0.999, decay_ph: 0.0})    \n",
    "    return adam\n",
    "\n",
    "def meta_opt():\n",
    "    meta_optimizer.reset_states()\n",
    "    return meta_optimizer\n",
    "\n",
    "\n",
    "optimizer_factories = [meta_opt, sgd_opt, adam_opt]\n",
    "\n",
    "trainings_per_dataset = 4\n",
    "\n",
    "comparison_results = compare_optimizers(meta_dataset=meta_dataset,\n",
    "                                        optimizer_factories=optimizer_factories,\n",
    "                                        n_learner_batches=conf.n_learner_batches,\n",
    "                                        learner_batch_size=conf.learner_batch_size,\n",
    "                                        learner=learner,\n",
    "                                        trainings_per_dataset=trainings_per_dataset,\n",
    "                                        initial_learner_weights=initial_learner_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "comparison_results = [np.array(r) for r in comparison_results]\n",
    "best_sgd, best_meta, best_adam = 0, 0, 0\n",
    "\n",
    "for i in range(len(comparison_results[0])):\n",
    "    res = [comparison_results[0][i], comparison_results[1][i], comparison_results[2][i]]\n",
    "    min_ind = res.index(min(res))\n",
    "    if min_ind == 0:\n",
    "        best_meta += 1\n",
    "    elif min_ind == 1:\n",
    "        best_sgd += 1\n",
    "    elif min_ind == 2:\n",
    "        best_adam += 1\n",
    "        \n",
    "denom = 100.0/len(comparison_results[0])\n",
    "print(\"SGD was best in {:d} trainings ({:.2f}%)\".format(best_sgd, best_sgd * denom))\n",
    "print(\"Adam was best in {:d} trainings ({:.2f}%)\".format(best_adam, best_adam * denom))\n",
    "print(\"META was best in {:d} trainings ({:.2f}%)\".format(best_meta, best_meta * denom))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "plt.pie([best_sgd, best_adam, best_meta], labels=['SGD', 'Adam', 'META'])\n",
    "plt.title(\"Best trainings (in terms of valid loss)\")\n",
    "plt.show()\n",
    "\n",
    "sgd_vs_adam = [sgd - adam for sgd, adam in zip(comparison_results[1], comparison_results[2])]\n",
    "meta_vs_sgd = [meta - sgd for meta, sgd in zip(comparison_results[0], comparison_results[1])]\n",
    "meta_vs_adam = [meta - adam for meta, adam in zip(comparison_results[0], comparison_results[2])]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "\n",
    "plt.hist(sgd_vs_adam, bins=100)\n",
    "plt.title(\"SGD vs ADAM (loss with sgd - loss with adam)\")\n",
    "plt.show()\n",
    "\n",
    "worse_count = sum(1 if s > 0 else 0 for s in sgd_vs_adam)\n",
    "worse_percent = worse_count / len(sgd_vs_adam) * 100\n",
    "print(\"SGD was worse than ADAM in {}% trainings\".format(worse_percent))\n",
    "\n",
    "plt.hist(meta_vs_sgd, bins=100)\n",
    "plt.title(\"META vs SGD (loss with meta - loss with sgd)\")\n",
    "plt.show()\n",
    "\n",
    "worse_count = sum(1 if s > 0 else 0 for s in meta_vs_sgd)\n",
    "worse_percent = worse_count / len(meta_vs_sgd) * 100\n",
    "print(\"META was worse than SGD in {}% trainings\".format(worse_percent))\n",
    "\n",
    "plt.hist(meta_vs_adam, bins=100)\n",
    "plt.title(\"META vs ADAM (loss with meta - loss with adam)\")\n",
    "plt.show()\n",
    "\n",
    "worse_count = sum(1 if s > 0 else 0 for s in meta_vs_adam)\n",
    "worse_percent = worse_count / len(meta_vs_adam) * 100\n",
    "print(\"META was worse than ADAM in {}% trainings\".format(worse_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
