{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:17:48.668843Z",
     "start_time": "2018-04-19T14:17:48.663270Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "    \n",
    "def moving_average(l, n=10):\n",
    "    cumsum, moving_aves = [0], []\n",
    "    for i, x in enumerate(l, 1):\n",
    "        cumsum.append(cumsum[i-1] + x)\n",
    "        if i>=n:\n",
    "            moving_ave = (cumsum[i] - cumsum[i-n])/n\n",
    "            moving_aves.append(moving_ave)\n",
    "    return moving_aves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:17:49.673816Z",
     "start_time": "2018-04-19T14:17:49.604746Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_train_train_loss, meta_train_valid_loss = [], []\n",
    "meta_train_train_acc, meta_train_valid_acc = [], []\n",
    "meta_valid_train_loss, meta_valid_valid_loss = [], []\n",
    "meta_valid_train_acc, meta_valid_valid_acc = [], []\n",
    "\n",
    "meta_train_loss_ratio = []\n",
    "meta_train_acc_ratio = []\n",
    "meta_valid_loss_ratio = []\n",
    "meta_valid_acc_ratio = []\n",
    "\n",
    "\n",
    "for meta in open('../log/meta_training_history.txt'): \n",
    "    meta = meta.split()\n",
    "    title = meta[0]\n",
    "    meta_results = meta[1].split(',')\n",
    "    if title == \"META_TRAIN_TRAIN:\":\n",
    "        meta_train_train_loss.append(float(meta_results[0]))\n",
    "        meta_train_train_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_TRAIN_VALID:\":\n",
    "        meta_train_valid_loss.append(float(meta_results[0]))\n",
    "        meta_train_valid_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_VALID_TRAIN:\": \n",
    "        meta_valid_train_loss.append(float(meta_results[0]))\n",
    "        meta_valid_train_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_VALID_VALID:\":\n",
    "        meta_valid_valid_loss.append(float(meta_results[0]))\n",
    "        meta_valid_valid_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_TRAIN_RATIO:\":\n",
    "        meta_train_loss_ratio.append(float(meta_results[0]))\n",
    "        meta_train_acc_ratio.append(float(meta_results[1]))\n",
    "    elif title == \"META_VALID_RATIO:\": \n",
    "        meta_valid_loss_ratio.append(float(meta_results[0]))\n",
    "        meta_valid_acc_ratio.append(float(meta_results[1]))\n",
    "        \n",
    "avg_n = 8\n",
    "\n",
    "meta_train_train_loss = moving_average(meta_train_train_loss, n=avg_n)\n",
    "meta_train_valid_loss = moving_average(meta_train_valid_loss, n=avg_n)\n",
    "meta_train_train_acc = moving_average(meta_train_train_acc, n=avg_n)\n",
    "meta_train_valid_acc = moving_average(meta_train_valid_acc, n=avg_n)\n",
    "meta_train_loss_ratio = moving_average(meta_train_loss_ratio, n=avg_n)\n",
    "meta_train_acc_ratio = moving_average(meta_train_acc_ratio, n=avg_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:17:51.977777Z",
     "start_time": "2018-04-19T14:17:50.796977Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_conf_log = []\n",
    "i = 0\n",
    "for line in open('../log/train-meta-model.log'):\n",
    "    if line.startswith('2018'):\n",
    "        if i == 0 : i += 1\n",
    "        else: break\n",
    "    else:\n",
    "        training_conf_log.append(line)\n",
    "        if line.startswith('lr_schedule:'):\n",
    "            lr_schedule = line.split('lr_schedule:')[-1]\n",
    "            lr_schedule = eval(lr_schedule)\n",
    "\n",
    "print('TRAINING CONFIGURATION:')\n",
    "print(''.join(training_conf_log))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "\n",
    "\n",
    "def draw_figure(train_data, valid_data, title, min_fun = None, train_label='train', valid_label='valid'):\n",
    "    train, = plt.plot(train_data, '-o', label=train_label)\n",
    "    legend_handles = [train]\n",
    "    if valid_data is not None:\n",
    "        valid, = plt.plot(valid_data, '-o', label=valid_label)\n",
    "        legend_handles.append(valid)\n",
    "    if valid_data is not None and min_fun is not None:\n",
    "        shift = 0.01\n",
    "        if min_fun is min:\n",
    "            shift *= -1\n",
    "        begin_handle, = plt.plot([0], [valid_data[0]], marker='o', markersize=8, color=\"orange\", label='starting')\n",
    "        ax = plt.gca()\n",
    "        ax.annotate('{:.5f}'.format(valid_data[0]), xy=(0, valid_data[0]), xytext=(-0.5, valid_data[0] + shift), \n",
    "                    horizontalalignment='left', verticalalignment='top')\n",
    "        \n",
    "        \n",
    "        best = min_fun(valid_data)\n",
    "        best_ind = valid_data.index(best)\n",
    "        best_handle, = plt.plot([best_ind], [best], marker='o', markersize=8, color=\"green\", label='best')\n",
    "        ax = plt.gca()\n",
    "        ax.annotate('{:.5f}'.format(best), xy=(best_ind, best), xytext=(best_ind - 0.5, best + shift), \n",
    "                    horizontalalignment='left', verticalalignment='top')\n",
    "        \n",
    "        legend_handles += [begin_handle, best_handle]\n",
    "\n",
    "    plt.legend(handles=legend_handles)\n",
    "    plt.title(title)\n",
    "    plt.show()    \n",
    "    \n",
    "plt.title(\"Meta-LR Schedule\")\n",
    "plt.step([lr[0] for lr in lr_schedule], [lr[1] for lr in lr_schedule], where='post')\n",
    "plt.xlabel(\"Meta-Epoch\")\n",
    "plt.ylabel(\"LR of optimizer (Adam) of Meta-Learner\")\n",
    "plt.show()\n",
    "    \n",
    "draw_figure(meta_train_train_loss, meta_train_valid_loss,\n",
    "           \"Average meta-batch loss (meta-train, moving average n = {})\".format(avg_n), min)\n",
    "\n",
    "draw_figure(meta_train_train_acc, meta_train_valid_acc,\n",
    "           \"Average meta-batch accuracy (meta-train, moving average n = {})\".format(avg_n), max)\n",
    "\n",
    "draw_figure(meta_train_loss_ratio, None,\n",
    "           \"Average meta-batch ratio of valid loss to train loss (meta-train, moving average n = {})\".format(avg_n),\n",
    "           train_label='valid/train ratio loss ratio (meta-train)')\n",
    "\n",
    "draw_figure(meta_valid_train_loss, meta_valid_valid_loss, \"Average loss (meta-valid)\", min)\n",
    "\n",
    "draw_figure(meta_valid_train_acc, meta_valid_valid_acc, \"Average accuracy (meta-valid)\", max)\n",
    "\n",
    "draw_figure(meta_valid_loss_ratio, None,\n",
    "           \"Average ratio of valid loss to train loss (meta-valid)\".format(avg_n),\n",
    "           train_label='valid/train ratio loss ratio (meta-train)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:18:05.737946Z",
     "start_time": "2018-04-19T14:18:02.291143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eigenvals analysis\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "eigen = {}\n",
    "\n",
    "d = '../log/eigenvals'\n",
    "for eigen_dir in (o for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))):\n",
    "    epoch = int(eigen_dir.split('_')[-1])\n",
    "    eigen_path = os.path.join(d, eigen_dir)\n",
    "    for eigen_file in os.listdir(os.path.join(d, eigen_dir)):\n",
    "        step = int(eigen_file.split('_')[1])\n",
    "        file_path = os.path.join(eigen_path, eigen_file)\n",
    "        vals = np.load(file_path)\n",
    "        if epoch not in eigen:\n",
    "            eigen[epoch] = {}\n",
    "        eigen[epoch][step] = np.mean(vals['E'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:18:05.872791Z",
     "start_time": "2018-04-19T14:18:05.738812Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_eigenvals = []\n",
    "\n",
    "for epoch in sorted(eigen.keys()):\n",
    "    avg = np.mean(list(eigen[epoch].values()))\n",
    "    avg_eigenvals.append(avg)\n",
    "        \n",
    "plt.plot(avg_eigenvals)\n",
    "\n",
    "plt.title(\"Average absolute values of first 3 eigenvals of minimum\")\n",
    "plt.xlabel('Meta-Validation epoch')\n",
    "plt.ylabel('Average absolute value of 1st eigenvalue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning/Forget rate analysis\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "avg_f, std_f = [], []\n",
    "avg_lr, std_lr = [], []\n",
    "\n",
    "d = '../log/stats'\n",
    "epochs = 0\n",
    "for stats_dir in sorted((o for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))),\n",
    "                       key=lambda d: int(d.split('_')[-1])):\n",
    "    if stats_dir.endswith('0'):\n",
    "        continue\n",
    "    avg_f_ep, std_f_ep = 0, 0\n",
    "    avg_lr_ep, std_lr_ep = 0, 0\n",
    "    n_steps = 0\n",
    "    epoch = int(stats_dir.split('_')[-1])\n",
    "    stats_path = os.path.join(d, stats_dir)\n",
    "    for stats_file in os.listdir(os.path.join(d, stats_dir)):\n",
    "        step = int(stats_file.split('_')[1])\n",
    "        file_path = os.path.join(stats_path, stats_file)\n",
    "        vals = np.load(file_path)\n",
    "        \n",
    "        avg_f_ep += np.mean(vals['forget_rate_history'])\n",
    "        avg_lr_ep += np.mean(vals['learning_rate_history'])\n",
    "        std_f_ep += np.std(vals['forget_rate_history'])\n",
    "        std_lr_ep += np.std(vals['learning_rate_history'])\n",
    "        \n",
    "        n_steps += 1\n",
    "    # ignore epochs that are not yet run\n",
    "    if n_steps < 64:\n",
    "        continue\n",
    "    avg_f_ep /= n_steps\n",
    "    avg_lr_ep /= n_steps\n",
    "    std_f_ep /= n_steps\n",
    "    std_lr_ep /= n_steps\n",
    "    \n",
    "    avg_f.append(avg_f_ep)\n",
    "    avg_lr.append(avg_lr_ep)\n",
    "    std_f.append(std_f_ep)\n",
    "    std_lr.append(std_lr_ep)\n",
    "    \n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_lower_draw = [avg - 0.5*std for avg, std in zip(avg_f, std_f)]\n",
    "std_upper_draw = [avg + 0.5*std for avg, std in zip(avg_f, std_f)]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Average forget rates\")\n",
    "plt.xlabel('Meta-Validation epoch')\n",
    "plt.ylabel('Average forget rate')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(avg_f,  c='b', label='avg_f', linewidth=2.0)\n",
    "ax.plot(std_upper_draw, c='r', label='avg_f + 0.5*std', linewidth=0.5)\n",
    "ax.plot(std_lower_draw, c='r', label='avg_f - 0.5*std', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "std_lower_draw = [avg - 0.5*std for avg, std in zip(avg_lr, std_lr)]\n",
    "std_upper_draw = [avg + 0.5*std for avg, std in zip(avg_lr, std_lr)]\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.title(\"Average learning rates\")\n",
    "plt.xlabel('Meta-Validation epoch')\n",
    "plt.ylabel('Average learning rate')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(avg_lr,  c='b', label='avg_lr', linewidth=2.0)\n",
    "ax.plot(std_upper_draw, c='r', label='avg_lr + 0.5*std', linewidth=0.5)\n",
    "ax.plot(std_lower_draw, c='r', label='avg_lr - 0.5*std', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for comparison of meta-optimizer with SGD and Adam\n",
    "\n",
    "import os\n",
    "from src.datasets.cifar import load_cifar100, cifar_input_shape\n",
    "from src.datasets.metadataset import load_meta_dataset\n",
    "\n",
    "from src.training.training_configuration import read_configuration\n",
    "from src.model.learner.simple_cnn import build_simple_cnn\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "train_conf_path = os.path.join(os.environ['CONF_DIR'], 'training_configuration.yml')\n",
    "conf = read_configuration(train_conf_path)\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_cifar100()\n",
    "\n",
    "meta_dataset_path = '../data/cifar100_64_64_2.h5'\n",
    "meta_dataset = load_meta_dataset(meta_dataset_path, X_train)\n",
    "\n",
    "learner = build_simple_cnn(cifar_input_shape, conf.classes_per_learner_set)\n",
    "\n",
    "best_sgd_lr = 0.05\n",
    "best_adam_lr = 0.04\n",
    "\n",
    "learner.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.0))  # dummy optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best LR for SGD optimizer\n",
    "\n",
    "from src.utils.comparison import learning_rate_grid_search\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "trainings_per_dataset = 4\n",
    "\n",
    "sgd = SGD()\n",
    "lr_ph = tf.placeholder(shape=sgd.lr.get_shape(), dtype=tf.float32)\n",
    "lr_update = K.update(sgd.lr, lr_ph)\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "def sgd_factory(lr):\n",
    "    # set LR in tensorflow\n",
    "    sess.run([lr_update], {lr_ph: lr})\n",
    "    return sgd\n",
    "\n",
    "best_sgd_lr = learning_rate_grid_search(\n",
    "                 optimizer_factory=sgd_factory,\n",
    "                 meta_dataset=meta_dataset,\n",
    "                 lr_values=[0.035, 0.04, 0.045],\n",
    "                 n_learner_batches=conf.n_learner_batches,\n",
    "                 learner_batch_size=conf.learner_batch_size,\n",
    "                 learner=learner,\n",
    "                 trainings_per_dataset=trainings_per_dataset)\n",
    "\n",
    "print(\"Best SGD LR:\", best_sgd_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best LR for Adam optimizer\n",
    "\n",
    "from src.utils.comparison import learning_rate_grid_search\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "trainings_per_dataset = 4\n",
    "\n",
    "adam = Adam()\n",
    "\n",
    "# placeholders and tensors for updating Adam states\n",
    "iterations_ph = tf.placeholder(shape=adam.iterations.get_shape(), dtype=tf.int64)\n",
    "lr_ph = tf.placeholder(shape=adam.lr.get_shape(), dtype=tf.float32)\n",
    "beta_1_ph = tf.placeholder(shape=adam.beta_1.get_shape(), dtype=tf.float32)\n",
    "beta_2_ph = tf.placeholder(shape=adam.beta_2.get_shape(), dtype=tf.float32)\n",
    "decay_ph = tf.placeholder(shape=adam.decay.get_shape(), dtype=tf.float32)\n",
    "\n",
    "iterations_update = K.update(adam.iterations, iterations_ph)\n",
    "lr_update = K.update(adam.lr, lr_ph)\n",
    "beta_1_update = K.update(adam.beta_1, beta_1_ph)\n",
    "beta_2_update = K.update(adam.beta_2, beta_2_ph)\n",
    "decay_update = K.update(adam.decay, decay_ph)\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "updates = [iterations_update, lr_update, beta_1_update, beta_2_update, decay_update]\n",
    "\n",
    "def adam_factory(lr):\n",
    "    # set LR and reset states in tensorflow\n",
    "\n",
    "    # default Adam values\n",
    "    sess.run(updates, {iterations_ph: 0, lr_ph: lr, beta_1_ph: 0.9, beta_2_ph: 0.999, decay_ph: 0.0})\n",
    "    return adam\n",
    "    \n",
    "best_adam_lr = learning_rate_grid_search(\n",
    "                 optimizer_factory=adam_factory,\n",
    "                 meta_dataset=meta_dataset,\n",
    "                 lr_values=[0.03, 0.04, 0.05],\n",
    "                 n_learner_batches=conf.n_learner_batches,\n",
    "                 learner_batch_size=conf.learner_batch_size,\n",
    "                 learner=learner,\n",
    "                 trainings_per_dataset=trainings_per_dataset)\n",
    "\n",
    "print(\"Best Adam LR:\", best_adam_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.comparison import compare_optimizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from src.model.meta_learner.lstm_model import lstm_meta_learner\n",
    "from src.isotropy.lanczos import TopKEigenvaluesBatched\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "eigenvals_callback = TopKEigenvaluesBatched(K=4, batch_size=conf.learner_batch_size, logger=None,\n",
    "                                            save_dir=\"\", save_eigenv=1)\n",
    "eigenvals_callback.model = learner\n",
    "eigenvals_callback.compile()\n",
    "\n",
    "\n",
    "meta_model = lstm_meta_learner(learner, eigenvals_callback, conf)\n",
    "\n",
    "meta_model.predict_model.compile(loss='mae',  # we don't use loss here anyway\n",
    "                                 optimizer=SGD(lr=0.0))  # dummy optimizer\n",
    "\n",
    "best_meta_learner_weights_path = os.path.join(os.environ['LOG_DIR'], \"meta_weights_best.h5\")\n",
    "meta_model.load_weights(best_meta_learner_weights_path)\n",
    "\n",
    "meta_optimizer = meta_model.predict_model\n",
    "\n",
    "\n",
    "sgd = SGD()\n",
    "\n",
    "adam = Adam()\n",
    "iterations_ph = tf.placeholder(shape=adam.iterations.get_shape(), dtype=tf.int64)\n",
    "beta_1_ph = tf.placeholder(shape=adam.beta_1.get_shape(), dtype=tf.float32)\n",
    "beta_2_ph = tf.placeholder(shape=adam.beta_2.get_shape(), dtype=tf.float32)\n",
    "decay_ph = tf.placeholder(shape=adam.decay.get_shape(), dtype=tf.float32)\n",
    "\n",
    "iterations_update = K.update(adam.iterations, iterations_ph)\n",
    "beta_1_update = K.update(adam.beta_1, beta_1_ph)\n",
    "beta_2_update = K.update(adam.beta_2, beta_2_ph)\n",
    "decay_update = K.update(adam.decay, decay_ph)\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "def sgd_opt(train_x, train_y):\n",
    "    # no state to reset in SGD (we don't use momentum)\n",
    "    return sgd\n",
    "\n",
    "def adam_opt(train_x, train_y):\n",
    "    # default Adam state values (learning rate states constant)\n",
    "    sess.run([iterations_update, beta_1_update, beta_2_update, decay_update], \n",
    "             {iterations_ph: 0, beta_1_ph: 0.9, beta_2_ph: 0.999, decay_ph: 0.0})    \n",
    "    return adam\n",
    "\n",
    "def meta_opt(train_x, train_y):\n",
    "    eigenvals_callback.X = train_x\n",
    "    eigenvals_callback.y = train_y\n",
    "\n",
    "    meta_optimizer.reset_states()\n",
    "    return meta_optimizer\n",
    "\n",
    "\n",
    "optimizer_factories = [meta_opt, sgd_opt, adam_opt]\n",
    "\n",
    "trainings_per_dataset = 4\n",
    "\n",
    "comparison_results = compare_optimizers(meta_dataset=meta_dataset,\n",
    "                                        optimizer_factories=optimizer_factories,\n",
    "                                        n_learner_batches=conf.n_learner_batches,\n",
    "                                        learner_batch_size=conf.learner_batch_size,\n",
    "                                        learner=learner,\n",
    "                                        trainings_per_dataset=trainings_per_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "comparison_results = [np.array(r) for r in comparison_results]\n",
    "best_sgd, best_meta, best_adam = 0, 0, 0\n",
    "\n",
    "for i in range(len(comparison_results[0])):\n",
    "    res = [comparison_results[0][i], comparison_results[1][i], comparison_results[2][i]]\n",
    "    min_ind = res.index(min(res))\n",
    "    if min_ind == 0:\n",
    "        best_meta += 1\n",
    "    elif min_ind == 1:\n",
    "        best_sgd += 1\n",
    "    elif min_ind == 2:\n",
    "        best_adam += 1\n",
    "        \n",
    "denom = 100.0/len(comparison_results[0])\n",
    "print(\"SGD was best in {:d} trainings ({:.2f}%)\".format(best_sgd, best_sgd * denom))\n",
    "print(\"Adam was best in {:d} trainings ({:.2f}%)\".format(best_adam, best_adam * denom))\n",
    "print(\"META was best in {:d} trainings ({:.2f}%)\".format(best_meta, best_meta * denom))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "plt.pie([best_sgd, best_adam, best_meta], labels=['SGD', 'Adam', 'META'])\n",
    "plt.title(\"Best trainings (in terms of valid loss)\")\n",
    "plt.show()\n",
    "\n",
    "sgd_vs_adam = [sgd - adam for sgd, adam in zip(comparison_results[1], comparison_results[2])]\n",
    "meta_vs_sgd = [meta - sgd for meta, sgd in zip(comparison_results[0], comparison_results[1])]\n",
    "meta_vs_adam = [meta - adam for meta, adam in zip(comparison_results[0], comparison_results[2])]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "\n",
    "plt.hist(sgd_vs_adam, bins=64)\n",
    "plt.title(\"SGD vs ADAM (loss with sgd - loss with adam)\")\n",
    "plt.show()\n",
    "\n",
    "worse_count = sum(1 if s > 0 else 0 for s in sgd_vs_adam)\n",
    "worse_percent = worse_count / len(sgd_vs_adam) * 100\n",
    "print(\"SGD was worse than ADAM in {}% trainings\".format(worse_percent))\n",
    "\n",
    "plt.hist(meta_vs_sgd, bins=64)\n",
    "plt.title(\"META vs SGD (loss with meta - loss with sgd)\")\n",
    "plt.show()\n",
    "\n",
    "worse_count = sum(1 if s > 0 else 0 for s in meta_vs_sgd)\n",
    "worse_percent = worse_count / len(meta_vs_sgd) * 100\n",
    "print(\"META was worse than SGD in {}% trainings\".format(worse_percent))\n",
    "\n",
    "plt.hist(meta_vs_adam, bins=64)\n",
    "plt.title(\"META vs ADAM (loss with meta - loss with adam)\")\n",
    "plt.show()\n",
    "\n",
    "worse_count = sum(1 if s > 0 else 0 for s in meta_vs_adam)\n",
    "worse_percent = worse_count / len(meta_vs_adam) * 100\n",
    "print(\"META was worse than ADAM in {}% trainings\".format(worse_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
