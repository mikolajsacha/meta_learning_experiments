{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:17:48.668843Z",
     "start_time": "2018-04-19T14:17:48.663270Z"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "    \n",
    "def moving_average(l, n=10):\n",
    "    cumsum, moving_aves = [0], []\n",
    "    for i, x in enumerate(l, 1):\n",
    "        cumsum.append(cumsum[i-1] + x)\n",
    "        if i>=n:\n",
    "            moving_ave = (cumsum[i] - cumsum[i-n])/n\n",
    "            moving_aves.append(moving_ave)\n",
    "    return moving_aves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:17:49.673816Z",
     "start_time": "2018-04-19T14:17:49.604746Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_train_train_loss, meta_train_valid_loss = [], []\n",
    "meta_train_train_acc, meta_train_valid_acc = [], []\n",
    "meta_valid_train_loss, meta_valid_valid_loss = [], []\n",
    "meta_valid_train_acc, meta_valid_valid_acc = [], []\n",
    "\n",
    "meta_train_loss_ratio = []\n",
    "meta_train_acc_ratio = []\n",
    "meta_valid_loss_ratio = []\n",
    "meta_valid_acc_ratio = []\n",
    "\n",
    "base_log_dir = '../log'\n",
    "\n",
    "\n",
    "for meta in open(base_log_dir + '/meta_training_history.txt'): \n",
    "    meta = meta.split()\n",
    "    title = meta[0]\n",
    "    meta_results = meta[1].split(',')\n",
    "    if title == \"META_TRAIN_TRAIN:\":\n",
    "        meta_train_train_loss.append(float(meta_results[0]))\n",
    "        meta_train_train_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_TRAIN_VALID:\":\n",
    "        meta_train_valid_loss.append(float(meta_results[0]))\n",
    "        meta_train_valid_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_VALID_TRAIN:\": \n",
    "        meta_valid_train_loss.append(float(meta_results[0]))\n",
    "        meta_valid_train_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_VALID_VALID:\":\n",
    "        meta_valid_valid_loss.append(float(meta_results[0]))\n",
    "        meta_valid_valid_acc.append(float(meta_results[1]))\n",
    "    elif title == \"META_TRAIN_RATIO:\":\n",
    "        meta_train_loss_ratio.append(float(meta_results[0]))\n",
    "        meta_train_acc_ratio.append(float(meta_results[1]))\n",
    "    elif title == \"META_VALID_RATIO:\": \n",
    "        meta_valid_loss_ratio.append(float(meta_results[0]))\n",
    "        meta_valid_acc_ratio.append(float(meta_results[1]))\n",
    "        \n",
    "n_train_sets = 64\n",
    "meta_batch_size = 8\n",
    "meta_batches_per_epoch = n_train_sets//meta_batch_size\n",
    "        \n",
    "avg_n = meta_batches_per_epoch*4\n",
    "\n",
    "meta_train_train_loss = moving_average(meta_train_train_loss, n=avg_n)\n",
    "meta_train_valid_loss = moving_average(meta_train_valid_loss, n=avg_n)\n",
    "meta_train_train_acc = moving_average(meta_train_train_acc, n=avg_n)\n",
    "meta_train_valid_acc = moving_average(meta_train_valid_acc, n=avg_n)\n",
    "meta_train_loss_ratio = moving_average(meta_train_loss_ratio, n=avg_n)\n",
    "meta_train_acc_ratio = moving_average(meta_train_acc_ratio, n=avg_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:17:51.977777Z",
     "start_time": "2018-04-19T14:17:50.796977Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_conf_log = {}\n",
    "\n",
    "for line in open(base_log_dir + '/train-meta-model.log'):\n",
    "    if not line.startswith('2018'):\n",
    "        split_line = line.strip().split(': ')\n",
    "        training_conf_log[split_line[0]] = split_line[1]\n",
    "        if line.startswith('lr_schedule:'):\n",
    "            lr_schedule = line.split('lr_schedule:')[-1]\n",
    "            lr_schedule = eval(lr_schedule)\n",
    "\n",
    "print('TRAINING CONFIGURATION:')\n",
    "print('\\n'.join(map(str, training_conf_log.items())))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "\n",
    "\n",
    "def draw_figure(train_data, valid_data, title, min_fun = None,\n",
    "                train_label='train', valid_label='valid', points_per_epoch=1):\n",
    "    train, = plt.plot(train_data, '-o', label=train_label)\n",
    "    legend_handles = [train]\n",
    "    if valid_data is not None:\n",
    "        valid, = plt.plot(valid_data, '-o', label=valid_label)\n",
    "        legend_handles.append(valid)\n",
    "        \n",
    "    ax = plt.gca()\n",
    "        \n",
    "    if valid_data is not None and min_fun is not None:\n",
    "        shift = 0.01\n",
    "        if min_fun is min:\n",
    "            shift *= -1\n",
    "        begin_handle, = plt.plot([0], [valid_data[0]], marker='o', markersize=8, color=\"orange\", label='starting')\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        ax.annotate('{:.5f}'.format(valid_data[0]), xy=(0, valid_data[0]), xytext=(-0.5, valid_data[0] + shift), \n",
    "                    horizontalalignment='left', verticalalignment='top')\n",
    "        \n",
    "        \n",
    "        best = min_fun(valid_data)\n",
    "        best_ind = valid_data.index(best)\n",
    "        print(\"Best at:\", best_ind)\n",
    "        best_handle, = plt.plot([best_ind], [best], marker='o', markersize=8, color=\"green\", label='best')\n",
    "        ax = plt.gca()\n",
    "        ax.annotate('{:.5f}'.format(best), xy=(best_ind, best), xytext=(best_ind - 0.5, best + shift), \n",
    "                    horizontalalignment='left', verticalalignment='top')\n",
    "        \n",
    "        legend_handles += [begin_handle, best_handle]\n",
    "        \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.set_ylabel('meta-learning rate', color='black')\n",
    "    lr_x = [lr[0] * points_per_epoch for lr in lr_schedule]\n",
    "    lr_x = [x for x in lr_x if x < len(train_data)]\n",
    "    lr_y = [lr[1] for lr in lr_schedule[0:len(lr_x)]]\n",
    "    ax2.set_yscale(\"log\", nonposy='clip')\n",
    "    ax2.step(lr_x, lr_y, where='post', color='black', linewidth='0.5')\n",
    "\n",
    "    plt.legend(handles=legend_handles)\n",
    "    plt.title(title)\n",
    "    plt.show()    \n",
    "    \n",
    "    \n",
    "draw_figure(meta_train_train_loss, meta_train_valid_loss,\n",
    "           \"Average meta-batch loss (meta-train, moving average n = {})\".format(avg_n), min,\n",
    "           points_per_epoch=meta_batches_per_epoch)\n",
    "\n",
    "draw_figure(meta_train_train_acc, meta_train_valid_acc,\n",
    "           \"Average meta-batch accuracy (meta-train, moving average n = {})\".format(avg_n), max,\n",
    "           points_per_epoch=meta_batches_per_epoch)\n",
    "\n",
    "draw_figure(meta_train_loss_ratio, None,\n",
    "           \"Average meta-batch ratio of valid loss to train loss (meta-train, moving average n = {})\".format(avg_n),\n",
    "           train_label='valid/train ratio loss ratio (meta-train)',\n",
    "           points_per_epoch=meta_batches_per_epoch)\n",
    "\n",
    "draw_figure(meta_valid_train_loss, meta_valid_valid_loss, \"Average loss (meta-valid)\", min)\n",
    "\n",
    "draw_figure(meta_valid_train_acc, meta_valid_valid_acc, \"Average accuracy (meta-valid)\", max)\n",
    "\n",
    "draw_figure(meta_valid_loss_ratio, None,\n",
    "           \"Average ratio of valid loss to train loss (meta-valid)\".format(avg_n),\n",
    "           train_label='valid/train ratio loss ratio (meta-train)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:18:05.737946Z",
     "start_time": "2018-04-19T14:18:02.291143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eigenvals analysis\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "eigen = {}\n",
    "\n",
    "d = base_log_dir + '/eigenvals'\n",
    "for eigen_dir in (o for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))):\n",
    "    epoch = int(eigen_dir.split('_')[-1])\n",
    "    eigen_path = os.path.join(d, eigen_dir)\n",
    "    for eigen_file in os.listdir(os.path.join(d, eigen_dir)):\n",
    "        step = int(eigen_file.split('_')[1])\n",
    "        file_path = os.path.join(eigen_path, eigen_file)\n",
    "        vals = np.load(file_path)\n",
    "        if epoch not in eigen:\n",
    "            eigen[epoch] = {}\n",
    "        eigen[epoch][step] = vals['E'][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T14:18:05.872791Z",
     "start_time": "2018-04-19T14:18:05.738812Z"
    }
   },
   "outputs": [],
   "source": [
    "eigenvals = [[], [], [], []]\n",
    "# std_eigenvals = []\n",
    "\n",
    "for epoch in sorted(eigen.keys()):\n",
    "    eig = [np.mean(list(v[i] for v in eigen[epoch].values())) for i in range(4)]\n",
    "    for i in range(4):\n",
    "        eigenvals[i].append(eig[i])\n",
    "    # std = np.std(list(eigen[epoch].values()))\n",
    "        \n",
    "# std_lower_draw = [avg - 0.5*std for avg, std in zip(avg_eigenvals, std_eigenvals)]\n",
    "# std_upper_draw = [avg + 0.5*std for avg, std in zip(avg_eigenvals, std_eigenvals)]\n",
    "\n",
    "for i in range(4):\n",
    "    plt.plot(eigenvals[i], label='eigenval {}'.format(i+1))\n",
    "plt.legend()\n",
    "\n",
    "# ax.plot(std_upper_draw, c='r', label='avg_eig + 0.5*std', linewidth=0.5)\n",
    "# ax.plot(std_lower_draw, c='r', label='avg_eig - 0.5*std', linewidth=0.5)\n",
    "\n",
    "plt.title(\"Average absolute values of 3 biggest eigenval of minimum\")\n",
    "plt.xlabel('Meta-Validation epoch')\n",
    "plt.ylabel('Average absolute value of 1st eigenvalue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning/Forget rate analysis\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "avg_f, std_f = [], []\n",
    "avg_lr, std_lr = [], []\n",
    "\n",
    "d = base_log_dir + '/stats'\n",
    "epochs = 0\n",
    "for stats_dir in sorted((o for o in os.listdir(d) if os.path.isdir(os.path.join(d,o))),\n",
    "                       key=lambda d: int(d.split('_')[-1])):\n",
    "    if stats_dir.endswith('0'):\n",
    "        continue\n",
    "    avg_f_ep, std_f_ep = 0, 0\n",
    "    avg_lr_ep, std_lr_ep = 0, 0\n",
    "    n_steps = 0\n",
    "    epoch = int(stats_dir.split('_')[-1])\n",
    "    stats_path = os.path.join(d, stats_dir)\n",
    "    for stats_file in os.listdir(os.path.join(d, stats_dir)):\n",
    "        step = int(stats_file.split('_')[1])\n",
    "        file_path = os.path.join(stats_path, stats_file)\n",
    "        vals = np.load(file_path)\n",
    "        \n",
    "        if 'forget_rate_history' in vals:\n",
    "            avg_f_ep += np.mean(vals['forget_rate_history'])\n",
    "            std_f_ep += np.std(vals['forget_rate_history'])\n",
    "        avg_lr_ep += np.mean(vals['learning_rate_history'])\n",
    "        std_lr_ep += np.std(vals['learning_rate_history'])\n",
    "        \n",
    "        n_steps += 1\n",
    "    # ignore epochs that are not yet run\n",
    "    if n_steps < 32:\n",
    "        continue\n",
    "    avg_f_ep /= n_steps\n",
    "    avg_lr_ep /= n_steps\n",
    "    std_f_ep /= n_steps\n",
    "    std_lr_ep /= n_steps\n",
    "    \n",
    "    avg_f.append(avg_f_ep)\n",
    "    avg_lr.append(avg_lr_ep)\n",
    "    std_f.append(std_f_ep)\n",
    "    std_lr.append(std_lr_ep)\n",
    "    \n",
    "    epochs += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_lower_draw = [avg - 0.5*std for avg, std in zip(avg_f, std_f)]\n",
    "std_upper_draw = [avg + 0.5*std for avg, std in zip(avg_f, std_f)]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Average forget rates\")\n",
    "plt.xlabel('Meta-Validation epoch')\n",
    "plt.ylabel('Average forget rate')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(avg_f,  c='b', label='avg_f', linewidth=2.0)\n",
    "ax.plot(std_upper_draw, c='r', label='avg_f + 0.5*std', linewidth=0.5)\n",
    "ax.plot(std_lower_draw, c='r', label='avg_f - 0.5*std', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "std_lower_draw = [avg - 0.5*std for avg, std in zip(avg_lr, std_lr)]\n",
    "std_upper_draw = [avg + 0.5*std for avg, std in zip(avg_lr, std_lr)]\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.title(\"Average learning rates\")\n",
    "plt.xlabel('Meta-Validation epoch')\n",
    "plt.ylabel('Average learning rate')\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(avg_lr,  c='b', label='avg_lr', linewidth=2.0)\n",
    "ax.plot(std_upper_draw, c='r', label='avg_lr + 0.5*std', linewidth=0.5)\n",
    "ax.plot(std_lower_draw, c='r', label='avg_lr - 0.5*std', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare data for comparison of meta-optimizer with SGD and Adam\n",
    "\n",
    "import os\n",
    "from src.datasets.cifar import load_cifar100, cifar_input_shape\n",
    "from src.datasets.metadataset import load_meta_dataset\n",
    "\n",
    "from src.training.training_configuration import read_configuration\n",
    "from src.model.learner.simple_cnn import build_simple_cnn\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "train_conf_path = os.path.join(os.environ['LOG_DIR'], 'training_configuration.yml')\n",
    "conf = read_configuration(train_conf_path)\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_cifar100()\n",
    "\n",
    "meta_dataset_path = '../data/cifar100_64_64_2.h5'\n",
    "meta_dataset = load_meta_dataset(meta_dataset_path, X_train)\n",
    "\n",
    "learner = build_simple_cnn(cifar_input_shape, conf.classes_per_learner_set)\n",
    "learner.load_weights(os.path.join(os.environ['CONF_DIR'], 'initial_learner_weights.h5'))\n",
    "initial_learner_weights = learner.get_weights()\n",
    "\n",
    "best_sgd_lr = 0.03\n",
    "best_adam_lr = 0.03\n",
    "\n",
    "learner.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.0),\n",
    "                metrics=['accuracy'])  # dummy optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find best LR for SGD optimizer\n",
    "\n",
    "from src.utils.comparison import learning_rate_grid_search\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "trainings_per_dataset = 4\n",
    "\n",
    "def sgd_factory(lr):\n",
    "    return SGD(lr=lr)\n",
    "\n",
    "best_sgd_lr = learning_rate_grid_search(\n",
    "                 optimizer_factory=sgd_factory,\n",
    "                 meta_dataset=meta_dataset,\n",
    "                 lr_values=[0.01, 0.03, 0.05],\n",
    "                 n_learner_batches=conf.n_learner_batches,\n",
    "                 learner_batch_size=conf.learner_batch_size,\n",
    "                 learner=learner,\n",
    "                 initial_learner_weights=initial_learner_weights,\n",
    "                 trainings_per_dataset=trainings_per_dataset)\n",
    "\n",
    "print(\"Best SGD LR:\", best_sgd_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best LR for Adam optimizer\n",
    "\n",
    "from src.utils.comparison import learning_rate_grid_search\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "trainings_per_dataset = 4\n",
    "\n",
    "def adam_factory(lr):\n",
    "    return Adam(lr=lr)\n",
    "    \n",
    "best_adam_lr = learning_rate_grid_search(\n",
    "                 optimizer_factory=adam_factory,\n",
    "                 meta_dataset=meta_dataset,\n",
    "                 lr_values=[0.025, 0.03, 0.035],\n",
    "                 n_learner_batches=conf.n_learner_batches,\n",
    "                 learner_batch_size=conf.learner_batch_size,\n",
    "                 learner=learner,\n",
    "                 initial_learner_weights=initial_learner_weights,\n",
    "                 trainings_per_dataset=trainings_per_dataset)\n",
    "\n",
    "print(\"Best Adam LR:\", best_adam_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.utils.comparison import compare_optimizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "from src.model.meta_learner.lstm_model import lstm_meta_learner\n",
    "from src.isotropy.lanczos import TopKEigenvaluesBatched\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "eigenvals_callback = TopKEigenvaluesBatched(K=4, batch_size=conf.learner_batch_size, logger=None,\n",
    "                                            save_dir=\"\", save_eigenv=1)\n",
    "eigenvals_callback.model = learner\n",
    "eigenvals_callback.compile()\n",
    "\n",
    "\n",
    "meta_model = lstm_meta_learner(learner, eigenvals_callback, conf)\n",
    "\n",
    "meta_model.predict_model.compile(loss='mae',  # we don't use loss here anyway\n",
    "                                 optimizer=SGD(lr=0.0))  # dummy optimizer\n",
    "\n",
    "best_meta_learner_weights_path = os.path.join(os.environ['LOG_DIR'], \"meta_weights.h5\")\n",
    "meta_model.load_weights(best_meta_learner_weights_path)\n",
    "\n",
    "meta_optimizer = meta_model.predict_model\n",
    "\n",
    "def sgd_opt(train_x, train_y):\n",
    "    return SGD(lr=best_sgd_lr)\n",
    "\n",
    "def adam_opt(train_x, train_y):\n",
    "    return Adam(lr=best_adam_lr)\n",
    "\n",
    "def meta_opt(train_x, train_y):\n",
    "    eigenvals_callback.X = train_x\n",
    "    eigenvals_callback.y = train_y\n",
    "\n",
    "    meta_optimizer.reset_states()\n",
    "    return meta_optimizer\n",
    "\n",
    "\n",
    "optimizer_factories = [meta_opt, sgd_opt, adam_opt]\n",
    "\n",
    "trainings_per_dataset = 4\n",
    "\n",
    "comparison_results = compare_optimizers(meta_dataset=meta_dataset,\n",
    "                                        optimizer_factories=optimizer_factories,\n",
    "                                        n_learner_batches=conf.n_learner_batches,\n",
    "                                        learner_batch_size=conf.learner_batch_size,\n",
    "                                        learner=learner,\n",
    "                                        initial_learner_weights=initial_learner_weights,\n",
    "                                        trainings_per_dataset=trainings_per_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "comparison_results = [np.array(r) for r in comparison_results]\n",
    "best_sgd, best_meta, best_adam = 0, 0, 0\n",
    "\n",
    "for i in range(len(comparison_results[0])):\n",
    "    res = [comparison_results[0][i], comparison_results[1][i], comparison_results[2][i]]\n",
    "    min_ind = res.index(min(res))\n",
    "    if min_ind == 0:\n",
    "        best_meta += 1\n",
    "    elif min_ind == 1:\n",
    "        best_sgd += 1\n",
    "    elif min_ind == 2:\n",
    "        best_adam += 1\n",
    "        \n",
    "denom = 100.0/len(comparison_results[0])\n",
    "print(\"SGD was best in {:d} trainings ({:.2f}%)\".format(best_sgd, best_sgd * denom))\n",
    "print(\"Adam was best in {:d} trainings ({:.2f}%)\".format(best_adam, best_adam * denom))\n",
    "print(\"META was best in {:d} trainings ({:.2f}%)\".format(best_meta, best_meta * denom))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "plt.pie([best_sgd, best_adam, best_meta], labels=['SGD', 'Adam', 'META'])\n",
    "plt.title(\"Best trainings (in terms of valid loss)\")\n",
    "plt.show()\n",
    "\n",
    "sgd_vs_adam = [sgd - adam for sgd, adam in zip(comparison_results[1], comparison_results[2])]\n",
    "meta_vs_sgd = [meta - sgd for meta, sgd in zip(comparison_results[0], comparison_results[1])]\n",
    "meta_vs_adam = [meta - adam for meta, adam in zip(comparison_results[0], comparison_results[2])]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "\n",
    "plt.hist(sgd_vs_adam, bins=64)\n",
    "plt.title(\"SGD vs ADAM (loss with sgd - loss with adam)\")\n",
    "plt.show()\n",
    "\n",
    "worse_count = sum(1 if s > 0 else 0 for s in sgd_vs_adam)\n",
    "worse_percent = worse_count / len(sgd_vs_adam) * 100\n",
    "print(\"SGD was worse than ADAM in {}% trainings\".format(worse_percent))\n",
    "\n",
    "plt.hist(meta_vs_sgd, bins=64)\n",
    "plt.title(\"META vs SGD (loss with meta - loss with sgd)\")\n",
    "plt.show()\n",
    "\n",
    "worse_count = sum(1 if s > 0 else 0 for s in meta_vs_sgd)\n",
    "worse_percent = worse_count / len(meta_vs_sgd) * 100\n",
    "print(\"META was worse than SGD in {}% trainings\".format(worse_percent))\n",
    "\n",
    "plt.hist(meta_vs_adam, bins=64)\n",
    "plt.title(\"META vs ADAM (loss with meta - loss with adam)\")\n",
    "plt.show()\n",
    "\n",
    "worse_count = sum(1 if s > 0 else 0 for s in meta_vs_adam)\n",
    "worse_percent = worse_count / len(meta_vs_adam) * 100\n",
    "print(\"META was worse than ADAM in {}% trainings\".format(worse_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.comparison import analyze_training\n",
    "import numpy as np\n",
    "\n",
    "def analyze_optimizer_training(optimizer_factory, trainings_per_dataset = 1):\n",
    "    train_losses, train_accuracies, valid_losses, valid_accuracies, hessian_eigen =\\\n",
    "                        analyze_training(meta_dataset=meta_dataset,\n",
    "                                         optimizer_factory=optimizer_factory,\n",
    "                                         n_learner_batches=conf.n_learner_batches,\n",
    "                                         learner_batch_size=conf.learner_batch_size,\n",
    "                                         learner=learner,\n",
    "                                         initial_learner_weights=initial_learner_weights,\n",
    "                                         trainings_per_dataset=trainings_per_dataset)\n",
    "    train_losses = np.array(train_losses)\n",
    "    train_accuracies = np.array(train_accuracies)\n",
    "    valid_losses = np.array(valid_losses)\n",
    "    valid_accuracies = np.array(valid_accuracies)\n",
    "    hessian_eigen = np.array(hessian_eigen)\n",
    "\n",
    "    for metric_name, set_name, data in [('loss', 'train', train_losses),\n",
    "                                        ('accuracy', 'train', train_accuracies),\n",
    "                                        ('loss', 'valid', valid_losses),\n",
    "                                        ('accuracy', 'valid', valid_accuracies),\n",
    "                                        ('hessian spectral norm', 'train', hessian_eigen)]:\n",
    "        avg = np.mean(data, axis=0)\n",
    "        std = np.std(data, axis=0)\n",
    "\n",
    "        std_lower_draw = [avg - 0.5*std for avg, std in zip(avg, std)]\n",
    "        std_upper_draw = [avg + 0.5*std for avg, std in zip(avg, std)]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.title(\"Average {} during training ({} set)\".format(metric_name, set_name))\n",
    "        plt.xlabel('Training step')\n",
    "        plt.ylabel('Average {} ({} set)'.format(metric_name, set_name))\n",
    "        ax = fig.add_subplot(111)\n",
    "        short_label = 'avg_{}_{}'.format(set_name, metric_name)\n",
    "        ax.plot(avg,  c='b', label=short_label, linewidth=2.0)\n",
    "        ax.plot(std_upper_draw, c='r', label=short_label + ' + 0.5*std', linewidth=0.5)\n",
    "        ax.plot(std_lower_draw, c='r', label=short_label + ' - 0.5*std', linewidth=0.5)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return train_losses, train_accuracies, valid_losses, valid_accuracies, hessian_eigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sgd_opt(train_x, train_y):\n",
    "    return SGD(lr=best_sgd_lr)\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print(\"ANALYSIS OF SGD OPTIMIZER\")\n",
    "print(\"*\" * 50)\n",
    "sgd_analysis = analyze_optimizer_training(sgd_opt, trainings_per_dataset = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def adam_opt(train_x, train_y):\n",
    "    return Adam(lr=best_sgd_lr)\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print(\"ANALYSIS OF ADAM OPTIMIZER\")\n",
    "print(\"*\" * 50)\n",
    "adam_analysis = analyze_optimizer_training(adam_opt, trainings_per_dataset = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.model.meta_learner.lstm_model import lstm_meta_learner\n",
    "from src.isotropy.lanczos import TopKEigenvaluesBatched\n",
    "\n",
    "eigenvals_callback = TopKEigenvaluesBatched(K=conf.hessian_eigenvalue_features, \n",
    "                                            batch_size=conf.learner_batch_size, logger=None,\n",
    "                                            save_dir=\"\", save_eigenv=1)\n",
    "eigenvals_callback.model = learner\n",
    "eigenvals_callback.compile()\n",
    "\n",
    "meta_model = lstm_meta_learner(learner, eigenvals_callback, conf)\n",
    "meta_model.predict_model.compile(loss='mae',  # we don't use loss here anyway\n",
    "                                 optimizer=SGD(lr=0.0))  # dummy optimizer\n",
    "\n",
    "best_meta_learner_weights_path = os.path.join(os.environ['LOG_DIR'], \"meta_weights.h5\")\n",
    "meta_model.load_weights(best_meta_learner_weights_path)\n",
    "\n",
    "meta_optimizer = meta_model.predict_model\n",
    "\n",
    "def meta_opt(train_x, train_y):\n",
    "    eigenvals_callback.X = train_x\n",
    "    eigenvals_callback.y = train_y\n",
    "\n",
    "    meta_optimizer.reset_states()\n",
    "    return meta_optimizer\n",
    "\n",
    "print(\"*\" * 50)\n",
    "print(\"ANALYSIS OF META OPTIMIZER\")\n",
    "print(\"*\" * 50)\n",
    "meta_analysis = analyze_optimizer_training(meta_opt, trainings_per_dataset = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_losses, train_accuracies, valid_losses, valid_accuracies, hessian_eigen = meta_analysis\n",
    "\n",
    "avg_train_losses = train_losses.mean(axis=0)\n",
    "avg_train_accuracies = train_accuracies.mean(axis=0)\n",
    "avg_valid_losses = valid_losses.mean(axis=0)\n",
    "avg_valid_accuracies = valid_accuracies.mean(axis=0)\n",
    "avg_hessian_eigen = hessian_eigen.mean(axis=0)\n",
    "\n",
    "df = pd.DataFrame({'train_loss': avg_train_losses, 'train_acc': avg_train_accuracies, \n",
    "                   'valid_loss': avg_valid_losses, 'valid_acc': avg_valid_accuracies,\n",
    "                   'hessian_eigen': avg_hessian_eigen})\n",
    "df.to_hdf('../log/learner_training_shape.h5', 'training_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
